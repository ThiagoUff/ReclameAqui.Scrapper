{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import math\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nlp = spacy.load('pt_core_news_md')\n",
    "\n",
    "stop_words = spacy.lang.pt.stop_words.STOP_WORDS\n",
    "punctuations = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence, removeTopWords, lemmatize):\n",
    "\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(str(sentence))\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    if lemmatize:\n",
    "        mytokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens]\n",
    "    #mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    if removeTopWords:\n",
    "        mytokens = [word for word in mytokens if word not in stop_words]\n",
    "\n",
    "    # Removing punctuations\n",
    "    mytokens = [str(word) for word in mytokens if str(word) not in punctuations]\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeChars(dataFrame, hashtags):\n",
    "    # Remove @ tags\n",
    "\n",
    "    dataFrame.comment = dataFrame.comment.str.replace(r'(@\\w*)', '', regex=True)\n",
    "\n",
    "    # Remove URL\n",
    "    dataFrame.comment = dataFrame.comment.str.replace(r\"http\\S+\", \"\", regex=True)\n",
    "\n",
    "    # Remove # tag\n",
    "    if (hashtags):\n",
    "        dataFrame.comment = dataFrame.comment.str.replace(r'#\\w+', \"\", regex=True)\n",
    "    # comp_df.tweet = comp_df.tweet.str.replace(r'#+',\"\")\n",
    "\n",
    "    # Remove all non-character\n",
    "    # comp_df.tweet = comp_df.tweet.str.replace(r\"[^a-zA-Z ]\",\"\")\n",
    "\n",
    "    # Remove extra space\n",
    "    dataFrame.comment = dataFrame.comment.str.replace(r'( +)', \" \", regex=True)\n",
    "    dataFrame.comment = dataFrame.comment.str.strip()\n",
    "\n",
    "    # Change to lowercase\n",
    "    dataFrame.comment = dataFrame.comment.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removeTopWords = True\n",
    "hashtags = False\n",
    "lemmatize = True\n",
    "df = pd.read_excel(\"..\\Dataset\\dataset.xlsx\")\n",
    "\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "anger_train = df[msk].copy()\n",
    "anger_test = df[~msk].copy()\n",
    "\n",
    "print(\"*****************************************************************************************\")\n",
    "print(\"Run sendo executada com o dataSet 'Hate Speech Twitter annotations' que contem \" + str(len(df)) + \" texts\")\n",
    "print(\"Dos quais, \" + str(len(anger_train)) + \" São para treinamento e \" + str(len(anger_test)) + \" são para teste\")\n",
    "print(\"Os Argumentos dessa run são, RemoverTopWords: \" + str(removeTopWords) + \" Remover HashTags: \" + str(hashtags) + \" Lematizar: \" + str(lemmatize))\n",
    "print(\"*****************************************************************************************\")\n",
    "\n",
    "anger_train.loc[:, 'is_test'] = 0\n",
    "anger_test.loc[:, 'is_test'] = 1\n",
    "\n",
    "comp_df = pd.concat([anger_train, anger_test])\n",
    "comp_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "comp_df = comp_df[['txt', 'has_anger', 'is_test']]\n",
    "comp_df.columns = ['comment', 'label', 'is_test']\n",
    "\n",
    "comp_df['label'] = comp_df.label.replace('S', 1)\n",
    "comp_df['label'] = comp_df.label.replace(math.nan, 0)\n",
    "\n",
    "removeChars(comp_df, hashtags)\n",
    "\n",
    "comp_df['corpus'] = [spacy_tokenizer(comment, removeTopWords, lemmatize) for comment in comp_df.comment]\n",
    "\n",
    "count = Counter()\n",
    "\n",
    "for cp in comp_df.corpus:\n",
    "    count += Counter(cp)\n",
    "\n",
    "print(\"As 5 palavras mais comuns do BOW são:\")\n",
    "print(count.most_common(5))\n",
    "\n",
    "print(comp_df.label.unique())\n",
    "for label in sorted(comp_df.label.unique()):\n",
    "    corpusInLabel = comp_df.corpus[comp_df.label == label]\n",
    "\n",
    "    count = Counter()\n",
    "    for cp in corpusInLabel:\n",
    "        count += Counter(cp)\n",
    "\n",
    "    print(\"As 5 palavras mais comuns do Label \" + str(label) + \" são:\")\n",
    "    print(count.most_common(5))\n",
    "\n",
    "print(\"As frequencias dos Labels são as seguintes:\")\n",
    "print(comp_df.label.value_counts())\n",
    "\n",
    "comp_df.corpus = comp_df.apply(lambda x: \" \".join(x.corpus), axis=1)\n",
    "print(comp_df.head())\n",
    "\n",
    "x_train = comp_df.corpus[comp_df.is_test == 0]\n",
    "y_train = comp_df.label[comp_df.is_test == 0]\n",
    "x_test = comp_df.corpus[comp_df.is_test == 1]\n",
    "y_test = comp_df.label[comp_df.is_test == 1]\n",
    "\n",
    "print(\"shape do treinamento\")\n",
    "print(x_train.shape)\n",
    "print(\"shape do teste\")\n",
    "print(x_test.shape)\n",
    "\n",
    "freq_vector = CountVectorizer(min_df=2, ngram_range=(1, 2)).fit(comp_df.corpus)\n",
    "\n",
    "x_train = freq_vector.transform(x_train)\n",
    "x_test = freq_vector.transform(x_test)\n",
    "\n",
    "classifier = LogisticRegression(max_iter=500)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred_train = classifier.predict(x_train)\n",
    "print(\"FScore do classificador em cima do treinamento\")\n",
    "print(precision_recall_fscore_support(y_train, y_pred_train, average='macro', zero_division=True))\n",
    "\n",
    "y_pred_test = classifier.predict(x_test)\n",
    "print(\"FScore do classificador em cima do Teste\")\n",
    "print(precision_recall_fscore_support(y_test, y_pred_test, average='macro', zero_division=True))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Matriz de confusão\")\n",
    "plot_cm(cm)\n",
    "\n",
    "print(\"Resultado do Cross Validation\")\n",
    "print(cross_val_score(LogisticRegression(random_state=42), x_train, y_train, cv=5, verbose=0, n_jobs=-1).mean())\n",
    "\n",
    "#Apenas separando execucoes\n",
    "print(\"--------------------------------------FIM DA EXECUCAO--------------------------------------\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
