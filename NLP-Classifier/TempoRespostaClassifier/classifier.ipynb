{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification , TrainingArguments, Trainer, TFBertModel\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "from datasets import load_dataset, concatenate_datasets, Value\n",
    "\n",
    "import evaluate \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_bool_function(example):\n",
    "    example[\"label\"] = 1 if example[\"VoltariaNegocio\"] == True else 0\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetLiveTim = load_dataset(\"json\", data_files=\"tim.json\",  split=\"train\")\n",
    "datasetTimCelular = load_dataset(\"json\", data_files=\"timCelular.json\",  split=\"train\")\n",
    "\n",
    "dataset = concatenate_datasets([datasetLiveTim, datasetTimCelular])\n",
    "# dataset = dataset.filter(lambda example: example[\"Nota\"] == False or example[\"Nota\"] == True)\n",
    "dataset = dataset.filter(lambda example: example[\"Nota\"] is not None)\n",
    "# datasetBool = dataset.map(preprocess_bool_function)\n",
    "\n",
    "dataset = dataset.rename_column(\"Nota\", \"label\")\n",
    "datasetBool = dataset.rename_column(\"Descricao\", \"text\")\n",
    "# datasetBool = datasetBool.rename_column(\"Titulo\", \"text\")\n",
    "datasetBool = datasetBool.remove_columns([\"_id\",\"VoltariaNegocio\", \"Titulo\", \"Localizacao\", \"Data\", \"Categoria\", \"Produto\", \"Problema\", \"Interacoes\", \"Status\", \"Resolvido\"])\n",
    "# datasetBool = datasetBool.remove_columns([\"_id\",\"VoltariaNegocio\", \"Titulo\", \"Localizacao\", \"Data\", \"Categoria\", \"Produto\", \"Problema\", \"Interacoes\", \"Status\", \"Resolvido\", \"Nota\"])\n",
    "\n",
    "dataset = datasetBool.train_test_split(test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features = dataset[\"train\"].features.copy()\n",
    "new_features[\"label\"] = Value(\"float\")\n",
    "dataset = dataset.cast(new_features)\n",
    "dataset[\"train\"].features\n",
    "\n",
    "new_features = dataset[\"test\"].features.copy()\n",
    "new_features[\"label\"] = Value(\"float\")\n",
    "dataset = dataset.cast(new_features)\n",
    "dataset[\"test\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length= 512, padding=\"max_length\")\n",
    "    \n",
    "tokenized_df = dataset.map(preprocess_function, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=1)\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit = 2,\n",
    "    save_strategy = 'no',\n",
    "    load_best_model_at_end=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_df[\"train\"],\n",
    "    eval_dataset=tokenized_df[\"test\"],\n",
    "    # tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
